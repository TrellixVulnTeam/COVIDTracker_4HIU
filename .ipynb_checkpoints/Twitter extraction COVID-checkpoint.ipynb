{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate twitterCOVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use tweepy to extract data from the twitter apis. Need twitter API account and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from secrets import *\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import jsonpickle\n",
    "import dataset\n",
    "from datafreeze import freeze\n",
    "\n",
    "# Consumer key authentication\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# Access key authentication\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Set up the API with the authentication handler\n",
    "api = API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample tweets that include keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class OriginalListener(StreamListener):\n",
    "    '''This a batch extractor, it extraccts tweets in batches as defined by the batch size parameter'''\n",
    "    def __init__(self, api = None, fprefix = 'streamer', foldername = \"StreamDir\", batchsize = 100000):\n",
    "        # set up API\n",
    "        self.api = api or API()\n",
    "        self.counter = 0 # number of tweets?\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('%s/%s_%s.json' % (foldername, self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if  'in_reply_to_status' in data:\n",
    "            self.on_status(data)\n",
    "        elif 'delete' in data:\n",
    "            delete = json.loads(data)['delete']['status']\n",
    "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
    "                return False\n",
    "        elif 'limit' in data:\n",
    "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
    "                return False\n",
    "        elif 'warning' in data:\n",
    "            warning = json.loads(data)['warnings']\n",
    "            print(\"WARNING: %s\" % warning['message'])\n",
    "            return\n",
    "\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.output.write(status)\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.batchsize: #tweet batch size\n",
    "            self.output.close()\n",
    "            self.output  = open('%s/%s_%s.json' % (foldername, self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "            self.counter = 0 # uncomment to keep streaming going\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_limit(self, track):\n",
    "        print(\"WARNING: Limitation notice received, tweets missed: %d\" % track)\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        return \n",
    "\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print(\"Timeout, sleeping for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    '''\n",
    "    This function is a custom tweepy.StreamListener function that given a valid API, \n",
    "    loads tweets in batches in real time \n",
    "    '''\n",
    "    def __init__(self, api = None, retrieve = \"custom\"):\n",
    "        self.api = api or API()\n",
    "        self.retrieve = retrieve #other option would be all\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        '''\n",
    "        This functions first opens the output json file then \"on_status\" which means when the API\n",
    "        gives out a tweet adds the json file and then adds 1 to the counter up to the tweet batchsize when \n",
    "        the storage file is closed and a new file is created and the counter is reset\n",
    "        '''\n",
    "    \n",
    "#         if status.retweeted_status:\n",
    "#             return # if file was a retweet then return nothing\n",
    "\n",
    "        UserID = status.id\n",
    "        UserDescription = status.user.description #all user info\n",
    "        UserAccountCreation = status.user.created_at\n",
    "        UserLocation = status.user.location\n",
    "        TweetTime = status.created_at\n",
    "        TweetText = status.text\n",
    "        TweetCoordinates = status.coordinates\n",
    "        TweetPlace = status.place\n",
    "        NumOfFollowers = status.user.followers_count\n",
    "        NumOfRetweets = status.retweet_count\n",
    "        blob = TextBlob(TweetText)\n",
    "        Sentiment = blob.sentiment\n",
    "\n",
    "        #json.dumps turns dictionary or json file into string\n",
    "        if TweetPlace is not None:\n",
    "            TweetPlace = jsonpickle.encode(TweetPlace)\n",
    "\n",
    "        if TweetCoordinates is not None:\n",
    "            TweetCoordinates = jsonpickle.encode(TweetCoordinates)\n",
    "\n",
    "        table = db[StreamSettings.TABLE_NAME]\n",
    "        try:\n",
    "            table.insert(dict(\n",
    "                UserID=UserID,\n",
    "                UserDescription=UserDescription,\n",
    "                UserAccountCreation=UserAccountCreation,\n",
    "                UserLocation=UserLocation,\n",
    "                TweetTime=TweetTime,\n",
    "                TweetText=TweetText,\n",
    "                TweetCoordinates=TweetCoordinates,\n",
    "                TweetPlace=TweetPlace,\n",
    "                NumOfFollowers=NumOfFollowers,\n",
    "                NumOfRetweets=NumOfRetweets,\n",
    "                Polarity=Sentiment.polarity,\n",
    "                Subjectivity=Sentiment.subjectivity\n",
    "            ))\n",
    "        except ProgrammingError as err:\n",
    "            print(err)\n",
    "            \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            #returning False in on_data disconnects the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now a json files has been created in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toStream = input(\"Stream: 1(Yes) or 0(No)? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "import StreamSettings\n",
    "db = dataset.connect(StreamSettings.CONNECTION_STRING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up words to track\n",
    "keywords_to_track = list(['corona'])\n",
    "foldername = \"StreamSupper\"\n",
    "if toStream == \"1\":\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    # Instantiate the SListener object \n",
    "    listen = MyListener(api)\n",
    "\n",
    "    # Instantiate the Stream object\n",
    "    stream = Stream(auth, listen)\n",
    "\n",
    "    # Begin collecting data\n",
    "    stream.filter(track = StreamSettings.TRACK_TERMS) # async allows to use different threads in case\n",
    "    # the current processor runs out of time\n",
    "else:\n",
    "    print(\"Not streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dataset.connect(StreamSettings.CONNECTION_STRING)\n",
    "result = db[\"tweets\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(result, format='csv', filename=StreamSettings.CSV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our json file and making sense of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "filedir = \"StreamDinner\"\n",
    "for file in os.listdir(filedir):\n",
    "    print(file)\n",
    "    for line in open(filedir+'/'+str(file), 'r'):\n",
    "        tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could either use tweet location (place attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = map(lambda x: x is not None, tweets.place)\n",
    "tweets.loc[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or user location (see user attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using twint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In bash:  twint -s covid -o twintTest.csv --csv --location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio # for compatibility of notebooks and twint\n",
    "nest_asyncio.apply()\n",
    "c = twint.Config()\n",
    "c.Location = True\n",
    "c.Limit = 3\n",
    "c.Pandas = True\n",
    "c.Search = \"covid\"\n",
    "\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = twint.storage.panda.Tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterCOVID",
   "language": "python",
   "name": "twittercovid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
